{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "# from astronomaly.base.base_dataset import Dataset\n",
    "\n",
    "\n",
    "class LightCurveDataset(object):\n",
    "    def __init__(self, time_col, brightness,brightness_err,filters, **kwargs):\n",
    "        \"\"\"\n",
    "        Reads in light curve data from file(s).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        filename : str\n",
    "            If a single file (of any time) is to be read from, the path can be\n",
    "            given using this kwarg. \n",
    "        directory : str\n",
    "            A directory can be given instead of an explicit list of files. The\n",
    "            child class will load all appropriate files in this directory.\n",
    "        list_of_files : list\n",
    "            Instead of the above, a list of files to be loaded can be\n",
    "            explicitly given.\n",
    "        output_dir : str\n",
    "            The directory to save the log file and all outputs to. Defaults to\n",
    "            './' \n",
    "        lower_mag : float, optional\n",
    "            Applies a cut to the data, excludes everything above this, by \n",
    "            default 1\n",
    "        upper_mag : int, optional\n",
    "            Applies a cut to the data, excludes everything below this, by \n",
    "            default 25\n",
    "        \"\"\"\n",
    "\n",
    "#         super().__init__(time_col=time_col, brightness=brightness,\n",
    "#                           brightness_err=brightness_err,filters=filters, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "        self.data_type = 'light_curve'\n",
    "        \n",
    "\n",
    "        self.metadata = pd.DataFrame(data=[])\n",
    "\n",
    "        ids = [f.split(os.sep)[-1] for f in self.files]\n",
    "        self.metadata = pd.DataFrame({'filepath': self.files}, index=ids)\n",
    "\n",
    "        \n",
    "\n",
    "        # This can be index with corresponding column names\n",
    "        self.time_col = time_col\n",
    "        self.brightness = brightness\n",
    "        self.brightness_err = brightness_err\n",
    "        self.filters = filters\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def read_lc_from_file(flpath,data_dict,header_nrows,delim_whitespace):\n",
    "\n",
    "        \n",
    "        '''Function to read the lc from the data\n",
    "        \n",
    "        Input:\n",
    "        \n",
    "        flpath: the location of the file\n",
    "        \n",
    "        data_dict: dictionary with the keys ('time','mag','mag_err','flux','flux_err','filters')\n",
    "                the user provides the values corresponding to the keys\n",
    "                e.g {'time':1}, were 1 is the time column index\n",
    "                \n",
    "        brightness_unit: Units used to measure the brightness\n",
    "                        can either be 'flux' or 'mags'\n",
    "                        \n",
    "        header_nrows: The number of rows the header covers\n",
    "        \n",
    "        delim_whitespace: True when the data is not separated by a comma, false otherwise\n",
    "                \n",
    "    Output:\n",
    "    \n",
    "    standardized pandas dataframe with lc data'''\n",
    "        \n",
    "        \n",
    "        # Reading-in the data\n",
    "        data = pd.read_csv(flpath,skiprows=header_nrows,delim_whitespace=delim_whitespace,header=None)\n",
    "        \n",
    "        \n",
    "        # ==================Magnitudes==================================\n",
    "        # ==============================================================\n",
    "        ID = data.iloc[:,data_dict['id']]\n",
    "        if 'mag' in data_dict.keys(): \n",
    "            \n",
    "            \n",
    "            # ============MUtliple Mag columns=========================\n",
    "            \n",
    "            # The case of multiple brightness columns        \n",
    "            if type(data_dict['mag']) == list:\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; mag1 = data.iloc[:,data_dict['mag'][0]];\n",
    "                mag2 = data.iloc[:,data_dict['mag'][1]]\n",
    "                \n",
    "                # Case where there are brightness error columns\n",
    "                if 'mag_err' in data_dict.keys():                \n",
    "                    \n",
    "                    mag_error = data.iloc[:,data_dict['mag_err']]\n",
    "                    # Creating a new dictionary for the columns above separate data\n",
    "                    standard_data = {'ID':ID,'time':time,'mag1':mag1,'mag2':mag2,'mag_error':mag_error}\n",
    "                    \n",
    "                # Case were there are no error columns\n",
    "                else:\n",
    "                    \n",
    "                    standard_data = {'ID':ID,'time':time,'mag1':mag1,'mag2':mag2}\n",
    "                            \n",
    "            \n",
    "            \n",
    "                    \n",
    "            # ============Column with Mag_filters and errors==========================\n",
    "            \n",
    "            # Including filters in dataframe\n",
    "            elif 'filters'in data_dict.keys() and 'mag_err' in data_dict.keys():\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; mag = data.iloc[:,data_dict['mag']];\n",
    "                mag_error = data.iloc[:,data_dict['mag_err']]\n",
    "\n",
    "                filters = data.iloc[:,data_dict['filters']]\n",
    "                standard_data = {'ID':ID,'time':time,'mag':mag,'mag_error':mag_error,'filters':filters}\n",
    "                \n",
    "            elif 'filters' in data_dict.keys():\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; mag = data.iloc[:,data_dict['mag']];\n",
    "        \n",
    "\n",
    "                filters = data.iloc[:,data_dict['filters']]\n",
    "                standard_data = {'ID':ID,'time':time,'mag':mag,'filters':filters}\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            #=================Single Mag Column with and with errors============================\n",
    "            \n",
    "            # Case of single brightness columns    \n",
    "            else:    \n",
    "                \n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; mag = data.iloc[:,data_dict['mag']]; \n",
    "                \n",
    "                if 'mag_err' in data_dict.keys():\n",
    "                    \n",
    "                    \n",
    "                    mag_error = data.iloc[:,data_dict['mag_err']]\n",
    "                    # Creating a new dictionary for the columns above separate data\n",
    "                    standard_data = {'ID':ID,'time':time,'mag':mag,'mag_error':mag_error}\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    standard_data = {'ID':ID,'time':time,'mag':mag}\n",
    "                    \n",
    "                    \n",
    "    #-----------------------------------------------------------------------------------------------------------------                \n",
    "                \n",
    "    #`````````````````````````````````````````````````````````````````````````````````````````````````````````````````      \n",
    "    #============================================Fluxes===============================================================\n",
    "    #=================================================================================================================\n",
    "    #`````````````````````````````````````````````````````````````````````````````````````````````````````````````````\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            \n",
    "            \n",
    "                    # ============MUtliple Mag columns=========================\n",
    "            \n",
    "            # The case of multiple brightness columns        \n",
    "            if type(data_dict['flux']) == list:\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; flux1 = data.iloc[:,data_dict['flux'][0]];\n",
    "                flux2 = data.iloc[:,data_dict['flux'][1]]\n",
    "                \n",
    "                # Case where there are brightness error columns\n",
    "                if 'flux_err' in data_dict.keys():                \n",
    "                    \n",
    "                    flux_error = data.iloc[:,data_dict['flux_err']]\n",
    "                    # Creating a new dictionary for the columns above separate data\n",
    "                    standard_data = {'ID':ID,'time':time,'flux1':flux1,'flux2':flux2,'flux_error':flux_error}\n",
    "                    \n",
    "                # Case were there are no error columns\n",
    "                else:\n",
    "                    \n",
    "                    standard_data = {'ID':ID,'time':time,'flux1':flux1,'flux2':flux2}\n",
    "                            \n",
    "            \n",
    "            \n",
    "                    \n",
    "            # ============Column with Mag_filters and errors==========================\n",
    "            \n",
    "            # Including filters in dataframe\n",
    "            elif 'filters'in data_dict.keys() and 'flux_err' in data_dict.keys():\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; flux = data.iloc[:,data_dict['flux']];\n",
    "                flux_error = data.iloc[:,data_dict['flux_err']]\n",
    "\n",
    "                filters = data.iloc[:,data_dict['filters']]\n",
    "                standard_data = {'ID':ID,'time':time,'flux':flux,'flux_error':flux_error,'filters':filters}\n",
    "                \n",
    "            elif 'filters' in data_dict.keys():\n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; flux = data.iloc[:,data_dict['flux']];\n",
    "        \n",
    "\n",
    "                filters = data.iloc[:,data_dict['filters']]\n",
    "                standard_data = {'ID':ID,'time':time,'flux':flux,'filters':filters}\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            #=================Single Mag Column with and with errors============================\n",
    "            \n",
    "            # Case of single brightness columns    \n",
    "            else:    \n",
    "                \n",
    "                \n",
    "                # Separatting the columns as per input dictionary\n",
    "                time = data.iloc[:,data_dict['time']]; flux = data.iloc[:,data_dict['flux']]; \n",
    "                \n",
    "                if 'flux_err' in data_dict.keys():\n",
    "                    \n",
    "                    \n",
    "                    flux_error = data.iloc[:,data_dict['flux_err']]\n",
    "                    # Creating a new dictionary for the columns above separate data\n",
    "                    standard_data = {'ID':ID,'time':time,'flux':flux,'flux_error':flux_error}\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    standard_data = {'ID':ID,'time':time,'flux':flux}\n",
    "            \n",
    "            \n",
    "        \n",
    "        return pd.DataFrame.from_dict(standard_data)\n",
    "            # return light_curve\n",
    "\n",
    "    def get_display_data(self, idx,flpath,data_dict,header_nrows,delim_whitespace):\n",
    "        \"\"\"\n",
    "        Returns a single instance of the dataset in a form that is ready to be\n",
    "        displayed by the web front end.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        idx : str\n",
    "            Index (should be a string to avoid ambiguity)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dict\n",
    "            json-compatible dictionary of the light curve data\n",
    "        \"\"\"\n",
    "        # print(id)\n",
    "        # ***** Need to extend this to deal with other bands\n",
    "#         time_col = 'time'\n",
    "\n",
    "        # All the standard columns are included here\n",
    "        data_col = ['time','mag','flux','mag1','mag2',\n",
    "                    'flux1','flux2','filters']\n",
    "\n",
    "        err_col = ['mag_error','flux_error']\n",
    "\n",
    "        out_dict = {}\n",
    "\n",
    "        metadata = self.metadata\n",
    "#         flpath = metadata[idx]['filepath'].iloc[0]\n",
    "\n",
    "        try:\n",
    "\n",
    "            \n",
    "            # Reading in the light curve data\n",
    "            light_curve = self.read_lc_from_file(flpath,data_dict,header_nrows,delim_whitespace)\n",
    "            light_curve = light_curve[light_curve['ID']==idx]\n",
    "            \n",
    "            # Data and error index \n",
    "            data_indx = [cl for cl in data_col if cl in light_curve.columns.values.tolist()] \n",
    "            err_indx = [cl for cl in err_col if cl in light_curve.columns.values.tolist()]\n",
    "\n",
    "            \n",
    "            # Getting both the data and error columns as per index\n",
    "            out_dict['data'] = light_curve[data_indx].values.tolist()\n",
    "            \n",
    "            \n",
    "            # Returns true if we have error columns\n",
    "            if err_col[0] in light_curve.columns.values.tolist() or err_col[1] in light_curve.columns.values.tolist():\n",
    "                lc_errs = light_curve[err_indx]\n",
    "                out_dict['errors'] = lc_errs.values.tolist()\n",
    "                \n",
    "             \n",
    "\n",
    "                \n",
    "\n",
    "        except (pd.errors.ParserError, pd.errors.EmptyDataError, FileNotFoundError) as e:\n",
    "            print('Error parsing file', flpath)\n",
    "            print('Error message:')\n",
    "            print(e)\n",
    "            out_dict = {'data': [], 'errors': []}\n",
    "\n",
    "        return out_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LightCurveDataset' object has no attribute 'files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-823f5c8e21db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLightCurveDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrightness\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbrightness_err\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'malem'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nkops'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'manybs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpath2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/malema/Desktop/Malema_UWC_Work/Data/test_set_batch1.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m md.get_display_data(flpath=path2,data_dict={'time':1,'flux':3,'id':0,'filters':2},\n\u001b[1;32m      5\u001b[0m                      delim_whitespace=False,header_nrows=1,idx=13)\n",
      "\u001b[0;32m<ipython-input-1-d4ec6f763f93>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, time_col, brightness, brightness_err, filters, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'filepath'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LightCurveDataset' object has no attribute 'files'"
     ]
    }
   ],
   "source": [
    "md = LightCurveDataset(time_col=1, brightness=2,brightness_err=3,filters=4,files=['malem','nkops','manybs'])\n",
    "path2 = '/home/malema/Desktop/Malema_UWC_Work/Data/test_set_batch1.csv'\n",
    "\n",
    "md.get_display_data(flpath=path2,data_dict={'time':1,'flux':3,'id':0,'filters':2},\n",
    "                     delim_whitespace=False,header_nrows=1,idx=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID', 'time', 'flux', 'flux_error', 'filters']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path2 = '/home/malema/Desktop/Malema_UWC_Work/Data/test_set_batch1.csv'\n",
    "\n",
    "lc = md.read_lc_from_file(flpath=path3 + fl,data_dict={'time':1,'flux':3,'flux_err':2,'id':0,'filters':2},\n",
    "                     delim_whitespace=True,header_nrows=2)\n",
    "\n",
    "\n",
    "lc.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_col = ['mag_error','flux_error','time']\n",
    "\n",
    "emp_dict = {}\n",
    "if err_col[0] in lc.columns.values.tolist() or err_col[2] in lc.columns.values.tolist():\n",
    "    \n",
    "    emp_dict['mal'] = [1,2,3,4]\n",
    "    \n",
    "if err_col[0] in lc.columns.values.tolist() or err_col[2] in lc.columns.values.tolist():\n",
    "    \n",
    "    emp_dict['zak'] = [1,2,3,4]\n",
    "    \n",
    "emp_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = LightCurveDataset(time_col=1, brightness=2,brightness_err=3,filters=4,files=['malem','nkops','manybs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "files = ['m','z','l','ok']\n",
    "\n",
    "ids = [f.split(os.sep)[-1] for f in files]\n",
    "\n",
    "metadata = pd.DataFrame({'filepath': files}, index=ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>l</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ok</th>\n",
       "      <td>ok</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   filepath\n",
       "m         m\n",
       "z         z\n",
       "l         l\n",
       "ok       ok"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from os import listdir\n",
    "# from os.path import isfile, join\n",
    "\n",
    "\n",
    "\n",
    "# directory = '/home/malema/Desktop/Malema_UWC_Work/Data/'\n",
    "\n",
    "# # if directory is given\n",
    "\n",
    "\n",
    "# # if file path given \n",
    "# metadata = pd.DataFrame({'filepath':ids},index=ids)\n",
    "\n",
    "# # if list of files given\n",
    "\n",
    "# ids = [f.split(os.sep)[-1] for f in self.list_of_files]\n",
    "\n",
    "# self.metadata = pd.DataFrame({'filepath': self.list_of_files}, index=ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filepath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20121012_02331333_O_CrabNebula_E.dat</th>\n",
       "      <td>20121012_02331333_O_CrabNebula_E.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_set_batch1.csv</th>\n",
       "      <td>test_set_batch1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOP_tutorial.ipynb</th>\n",
       "      <td>OOP_tutorial.ipynb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  filepath\n",
       "20121012_02331333_O_CrabNebula_E.dat  20121012_02331333_O_CrabNebula_E.dat\n",
       "test_set_batch1.csv                                    test_set_batch1.csv\n",
       "OOP_tutorial.ipynb                                      OOP_tutorial.ipynb"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "flpath = metadata[metadata['filepath']=='20121012_02331333_O_CrabNebula_E.dat']\n",
    "\n",
    "# fl = flpath.iloc[0].values[0]\n",
    "\n",
    "# fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['20121012_02331333_O_CrabNebula_E.dat'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flpath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "path2 = '/home/malema/Desktop/Malema_UWC_Work/Data/'\n",
    "ids = [f for f in listdir(path2) if isfile(join(path2, f))]\n",
    "dt = pd.DataFrame({'filename':ids},index=ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_set_batch1.csv'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flpath = dt.loc['test_set_batch1.csv','filename']\n",
    "\n",
    "flpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20121012_02331333_O_CrabNebula_E.dat</th>\n",
       "      <td>20121012_02331333_O_CrabNebula_E.dat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_set_batch1.csv</th>\n",
       "      <td>test_set_batch1.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OOP_tutorial.ipynb</th>\n",
       "      <td>OOP_tutorial.ipynb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  filename\n",
       "20121012_02331333_O_CrabNebula_E.dat  20121012_02331333_O_CrabNebula_E.dat\n",
       "test_set_batch1.csv                                    test_set_batch1.csv\n",
       "OOP_tutorial.ipynb                                      OOP_tutorial.ipynb"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
